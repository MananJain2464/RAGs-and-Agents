{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: arxiv in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.1.3)\n",
      "Requirement already satisfied: feedparser~=6.0.10 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from arxiv) (6.0.11)\n",
      "Requirement already satisfied: requests~=2.32.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from arxiv) (2.32.3)\n",
      "Requirement already satisfied: sgmllib3k in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from feedparser~=6.0.10->arxiv) (1.0.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests~=2.32.0->arxiv) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests~=2.32.0->arxiv) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests~=2.32.0->arxiv) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests~=2.32.0->arxiv) (2024.12.14)\n"
     ]
    }
   ],
   "source": [
    "! pip install arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools import WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wikipedia in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.4.0)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from wikipedia) (4.12.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from wikipedia) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2024.12.14)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from beautifulsoup4->wikipedia) (2.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_wrapper = WikipediaAPIWrapper(top_k_results=1, doc_content_chars_max=200)\n",
    "wiki = WikipediaQueryRun(api_wrapper=api_wrapper)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wikipedia'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain-community in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.3.18)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.37 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-community) (0.3.37)\n",
      "Requirement already satisfied: langchain<1.0.0,>=0.3.19 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-community) (0.3.19)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-community) (2.0.38)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-community) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-community) (6.0.2)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-community) (3.11.12)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-community) (9.0.0)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-community) (2.8.0)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.125 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-community) (0.3.9)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-community) (0.4.0)\n",
      "Requirement already satisfied: numpy<3,>=1.26.2 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-community) (2.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.4.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.18.3)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.6 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain<1.0.0,>=0.3.19->langchain-community) (0.3.6)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain<1.0.0,>=0.3.19->langchain-community) (2.10.3)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.37->langchain-community) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.37->langchain-community) (24.1)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.37->langchain-community) (4.12.2)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-community) (3.10.15)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-community) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.23.0)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.0.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2->langchain-community) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2->langchain-community) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2->langchain-community) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2->langchain-community) (2024.12.14)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.1.1)\n",
      "Requirement already satisfied: anyio in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (4.8.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.37->langchain-community) (3.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.19->langchain-community) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.19->langchain-community) (2.27.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.3.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install langchain-community\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x0000025380376450>, search_kwargs={})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader \n",
    "from langchain_community.vectorstores import FAISS \n",
    "from langchain_openai import OpenAIEmbeddings \n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "loader = WebBaseLoader('https://medium.com/tag/machine-learning') \n",
    "docs = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "documents = text_splitter.split_documents(docs)\n",
    "vectordb= FAISS.from_documents(documents, OpenAIEmbeddings())\n",
    "\n",
    "retriever = vectordb.as_retriever()\n",
    "retriever\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools.retriever import create_retriever_tool\n",
    "retriever_tool = create_retriever_tool(retriever, 'medium_search' , 'Search Medium FOR ANYTHING RELATED TO MACHINE LEARNING AND BUILD YOUR ANSWER OVER IT, ADD INFORMATION FROM YOUR SIDE IF YOU FIND SOMETHING MISSING ' )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'medium_search'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever_tool.name "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'arxiv'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.utilities import ArxivAPIWrapper\n",
    "from langchain_community.tools import ArxivQueryRun\n",
    "\n",
    "arxiv_wrapper = ArxivAPIWrapper(top_k_results=1 , doc_content_chars_max=200)\n",
    "arxiv = ArxivQueryRun(api_wrapper=arxiv_wrapper)\n",
    "arxiv.name "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper(wiki_client=<module 'wikipedia' from 'c:\\\\Users\\\\user\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\Lib\\\\site-packages\\\\wikipedia\\\\__init__.py'>, top_k_results=1, lang='en', load_all_available_meta=False, doc_content_chars_max=200)),\n",
       " ArxivQueryRun(api_wrapper=ArxivAPIWrapper(arxiv_search=<class 'arxiv.Search'>, arxiv_exceptions=(<class 'arxiv.ArxivError'>, <class 'arxiv.UnexpectedEmptyPageError'>, <class 'arxiv.HTTPError'>), top_k_results=1, ARXIV_MAX_QUERY_LENGTH=300, continue_on_failure=False, load_max_docs=100, load_all_available_meta=False, doc_content_chars_max=200)),\n",
       " Tool(name='medium_search', description='Search Medium FOR ANYTHING RELATED TO MACHINE LEARNING AND BUILD YOUR ANSWER OVER IT, ADD INFORMATION FROM YOUR SIDE IF YOU FIND SOMETHING MISSING ', args_schema=<class 'langchain_core.tools.retriever.RetrieverInput'>, func=functools.partial(<function _get_relevant_documents at 0x00000253E707A340>, retriever=VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x0000025380376450>, search_kwargs={}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_separator='\\n\\n', response_format='content'), coroutine=functools.partial(<function _aget_relevant_documents at 0x00000253E73B96C0>, retriever=VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x0000025380376450>, search_kwargs={}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_separator='\\n\\n', response_format='content'))]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools = [wiki , arxiv , retriever_tool ]\n",
    "tools "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv \n",
    "\n",
    "load_dotenv()\n",
    "import os \n",
    "os.environ['OPENAI_API_KEY'] = os.getenv(\"OPENAI_API_KEY\")\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model = 'gpt-3.5-turbo-0125' , temperature=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchainhub\n",
      "  Downloading langchainhub-0.1.21-py3-none-any.whl.metadata (659 bytes)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchainhub) (24.1)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchainhub) (2.32.3)\n",
      "Collecting types-requests<3.0.0.0,>=2.31.0.2 (from langchainhub)\n",
      "  Downloading types_requests-2.32.0.20241016-py3-none-any.whl.metadata (1.9 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2->langchainhub) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2->langchainhub) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2->langchainhub) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2->langchainhub) (2024.12.14)\n",
      "Downloading langchainhub-0.1.21-py3-none-any.whl (5.2 kB)\n",
      "Downloading types_requests-2.32.0.20241016-py3-none-any.whl (15 kB)\n",
      "Installing collected packages: types-requests, langchainhub\n",
      "Successfully installed langchainhub-0.1.21 types-requests-2.32.0.20241016\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install langchainhub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langsmith\\client.py:253: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a helpful assistant'), additional_kwargs={}),\n",
       " MessagesPlaceholder(variable_name='chat_history', optional=True),\n",
       " HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={}),\n",
       " MessagesPlaceholder(variable_name='agent_scratchpad')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import hub \n",
    "\n",
    "prompt = hub.pull('hwchase17/openai-functions-agent')\n",
    "prompt.messages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_openai_tools_agent \n",
    "agent = create_openai_tools_agent(llm , tools , prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AgentExecutor(verbose=True, agent=RunnableMultiActionAgent(runnable=RunnableAssign(mapper={\n",
       "  agent_scratchpad: RunnableLambda(lambda x: format_to_openai_tool_messages(x['intermediate_steps']))\n",
       "})\n",
       "| ChatPromptTemplate(input_variables=['agent_scratchpad', 'input'], optional_variables=['chat_history'], input_types={'chat_history': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x00000253E689C7C0>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]], 'agent_scratchpad': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x00000253E689C7C0>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]]}, partial_variables={'chat_history': []}, metadata={'lc_hub_owner': 'hwchase17', 'lc_hub_repo': 'openai-functions-agent', 'lc_hub_commit_hash': 'a1655024b06afbd95d17449f21316291e0726f13dcfaf990cc0d18087ad689a5'}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a helpful assistant'), additional_kwargs={}), MessagesPlaceholder(variable_name='chat_history', optional=True), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={}), MessagesPlaceholder(variable_name='agent_scratchpad')])\n",
       "| RunnableBinding(bound=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x00000253DCEEA7B0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x00000253DCEEBB30>, root_client=<openai.OpenAI object at 0x00000253FE765370>, root_async_client=<openai.AsyncOpenAI object at 0x00000253DCE76810>, model_name='gpt-3.5-turbo-0125', temperature=0.5, model_kwargs={}, openai_api_key=SecretStr('**********')), kwargs={'tools': [{'type': 'function', 'function': {'name': 'wikipedia', 'description': 'A wrapper around Wikipedia. Useful for when you need to answer general questions about people, places, companies, facts, historical events, or other subjects. Input should be a search query.', 'parameters': {'properties': {'query': {'description': 'query to look up on wikipedia', 'type': 'string'}}, 'required': ['query'], 'type': 'object'}}}, {'type': 'function', 'function': {'name': 'arxiv', 'description': 'A wrapper around Arxiv.org Useful for when you need to answer questions about Physics, Mathematics, Computer Science, Quantitative Biology, Quantitative Finance, Statistics, Electrical Engineering, and Economics from scientific articles on arxiv.org. Input should be a search query.', 'parameters': {'properties': {'query': {'description': 'search query to look up', 'type': 'string'}}, 'required': ['query'], 'type': 'object'}}}, {'type': 'function', 'function': {'name': 'medium_search', 'description': 'Search Medium FOR ANYTHING RELATED TO MACHINE LEARNING AND BUILD YOUR ANSWER OVER IT, ADD INFORMATION FROM YOUR SIDE IF YOU FIND SOMETHING MISSING ', 'parameters': {'properties': {'query': {'description': 'query to look up in retriever', 'type': 'string'}}, 'required': ['query'], 'type': 'object'}}}]}, config={}, config_factories=[])\n",
       "| OpenAIToolsAgentOutputParser(), input_keys_arg=[], return_keys_arg=[], stream_runnable=True), tools=[WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper(wiki_client=<module 'wikipedia' from 'c:\\\\Users\\\\user\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\Lib\\\\site-packages\\\\wikipedia\\\\__init__.py'>, top_k_results=1, lang='en', load_all_available_meta=False, doc_content_chars_max=200)), ArxivQueryRun(api_wrapper=ArxivAPIWrapper(arxiv_search=<class 'arxiv.Search'>, arxiv_exceptions=(<class 'arxiv.ArxivError'>, <class 'arxiv.UnexpectedEmptyPageError'>, <class 'arxiv.HTTPError'>), top_k_results=1, ARXIV_MAX_QUERY_LENGTH=300, continue_on_failure=False, load_max_docs=100, load_all_available_meta=False, doc_content_chars_max=200)), Tool(name='medium_search', description='Search Medium FOR ANYTHING RELATED TO MACHINE LEARNING AND BUILD YOUR ANSWER OVER IT, ADD INFORMATION FROM YOUR SIDE IF YOU FIND SOMETHING MISSING ', args_schema=<class 'langchain_core.tools.retriever.RetrieverInput'>, func=functools.partial(<function _get_relevant_documents at 0x00000253E707A340>, retriever=VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x0000025380376450>, search_kwargs={}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_separator='\\n\\n', response_format='content'), coroutine=functools.partial(<function _aget_relevant_documents at 0x00000253E73B96C0>, retriever=VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x0000025380376450>, search_kwargs={}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_separator='\\n\\n', response_format='content'))])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.agents import AgentExecutor \n",
    "agent_executer = AgentExecutor(agent = agent, tools = tools , verbose = True)\n",
    "agent_executer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `medium_search` with `{'query': 'Machine Learning'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[38;5;200m\u001b[1;3mThe most insightful stories about Machine Learning - MediumOpen in appSign upSign inWriteSign upSign inExplore topicsMachine LearningArtificial IntelligenceTechnologyData ScienceDeep LearningAIProgrammingPythonSelf ImprovementRelationshipsMachine LearningTopicÂ·5.4M FollowersÂ·372K StoriesFollowRecommended storiesValeriy Manokhin, PhD, MBA, CQFThe Makridakis Forecasting Competitions: Four Decades of Overhyped Simplicity and StagnationFor over four decades, the Makridakis forecasting competitions (M-competitions) have been held up as the â€œstandardâ€ for evaluatingâ€¦5h ago1InGusto EngineeringbyDenis KazakovTackling AI Hallucinations in LLM AppsHow Token Log-Probabilities Can Predict LLM Reliability5d ago5Cobus GreylingWhy The Focus Has Shifted from AI Agents to Agentic WorkflowsWe find ourselves on a stairway from where Large Language Models were introduced to AI Agents with human like digital interactions. Butâ€¦Feb 524Feb 524InTowards AIbyGao Dalie (é«˜é”çƒˆ)LangGraph + DeepSeek-R1 + Function\n",
      "\n",
      "on a stairway from where Large Language Models were introduced to AI Agents with human like digital interactions. Butâ€¦Feb 524Feb 524InTowards AIbyGao Dalie (é«˜é”çƒˆ)LangGraph + DeepSeek-R1 + Function Call + Agentic RAG (Insane Results)In this video, I have a super quick tutorial showing you how to create a multi-agent chatbot using LangGraph, Deepseek-R1, function calling,3h ago3h agoInAI MindbyFrancesco FrancoStyleGAN: a Step by Step IntroductionGenerative Adversarial Networks have been the go-to machine learning technique for generative content in the past few years. Seeminglyâ€¦2d ago2d agoValeriy Manokhin, PhD, MBA, CQFThe Makridakis Forecasting Competitions: Four Decades of Overhyped Simplicity and StagnationFor over four decades, the Makridakis forecasting competitions (M-competitions) have been held up as the â€œstandardâ€ for evaluatingâ€¦5h ago1InGusto EngineeringbyDenis KazakovTackling AI Hallucinations in LLM AppsHow Token Log-Probabilities Can Predict LLM Reliability5d ago5Cobus\n",
      "\n",
      "generative content in the past few years. Seeminglyâ€¦2d agoInCodeXbyAustin StarksGoogle just ANNIHILATED DeepSeek and OpenAI with their new Flash 2.0 modelThree weeks ago, when DeepSeek released R1, their inexpensive reasoning model, I thought it was the pinnacle of the AI revolution.Feb 549VishnusathwikBeautiful Math Behind Principle Component Analysis.ðŸ“Œ This blog is inspired by a lecture from the SMAI course by Prof. Vineet Gandhi on PCA.1d agoInLevel Up CodingbyBen ðŸ‰Here Is All You Need for Machine LearningÂ !!From Mathematical Foundations to Iris Classificationâ€Šâ€”â€ŠUnlock the Secrets of AIFeb 101See more recommended stories\n",
      "\n",
      "have been held up as the â€œstandardâ€ for evaluatingâ€¦5h ago1InGusto EngineeringbyDenis KazakovTackling AI Hallucinations in LLM AppsHow Token Log-Probabilities Can Predict LLM Reliability5d ago5Cobus GreylingWhy The Focus Has Shifted from AI Agents to Agentic WorkflowsWe find ourselves on a stairway from where Large Language Models were introduced to AI Agents with human like digital interactions. Butâ€¦Feb 524InTowards AIbyGao Dalie (é«˜é”çƒˆ)LangGraph + DeepSeek-R1 + Function Call + Agentic RAG (Insane Results)In this video, I have a super quick tutorial showing you how to create a multi-agent chatbot using LangGraph, Deepseek-R1, function calling,3h agoInAI MindbyFrancesco FrancoStyleGAN: a Step by Step IntroductionGenerative Adversarial Networks have been the go-to machine learning technique for generative content in the past few years. Seeminglyâ€¦2d agoInCodeXbyAustin StarksGoogle just ANNIHILATED DeepSeek and OpenAI with their new Flash 2.0 modelThree weeks ago, when DeepSeek released R1,\u001b[0m\u001b[32;1m\u001b[1;3mMachine learning is a fascinating field that involves the study and development of algorithms and models that enable computers to learn from and make predictions or decisions based on data. It is a subset of artificial intelligence (AI) and has applications in various domains such as data analysis, pattern recognition, natural language processing, and more.\n",
      "\n",
      "If you're interested in exploring more in-depth articles and insights on machine learning, I can help you find specific topics or resources. Just let me know what you'd like to learn more about!\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Tell me about Machine Learning',\n",
       " 'output': \"Machine learning is a fascinating field that involves the study and development of algorithms and models that enable computers to learn from and make predictions or decisions based on data. It is a subset of artificial intelligence (AI) and has applications in various domains such as data analysis, pattern recognition, natural language processing, and more.\\n\\nIf you're interested in exploring more in-depth articles and insights on machine learning, I can help you find specific topics or resources. Just let me know what you'd like to learn more about!\"}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executer.invoke({\"input\":\"Tell me about Machine Learning\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `wikipedia` with `{'query': 'LSTM architecture'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mPage: Long short-term memory\n",
      "Summary: Long short-term memory (LSTM) is a type of recurrent neural network (RNN) aimed at mitigating the vanishing gradient problem commonly encountered by traditional R\u001b[0m\u001b[32;1m\u001b[1;3mLong Short-Term Memory (LSTM) is a type of recurrent neural network (RNN) designed to address the vanishing gradient problem often encountered in traditional RNNs. LSTM architecture includes specialized units called memory blocks. Each block has components like gates that manage the flow of information, a cell state that serves as the memory, and an output. These components work together to allow LSTMs to store and access information over long sequences, making them effective for tasks involving long-term dependencies.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Tell me about the architecture of LSTM',\n",
       " 'output': 'Long Short-Term Memory (LSTM) is a type of recurrent neural network (RNN) designed to address the vanishing gradient problem often encountered in traditional RNNs. LSTM architecture includes specialized units called memory blocks. Each block has components like gates that manage the flow of information, a cell state that serves as the memory, and an output. These components work together to allow LSTMs to store and access information over long sequences, making them effective for tasks involving long-term dependencies.'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executer.invoke({\"input\":\"Tell me about the architecture of LSTM\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `wikipedia` with `{'query': 'Attention is all you need'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mPage: Attention Is All You Need\n",
      "Summary: \"Attention Is All You Need\" is a 2017 landmark research paper in machine learning authored by eight scientists working at Google. The paper introduced a new de\u001b[0m\u001b[32;1m\u001b[1;3mThe authors of the research paper \"Attention Is All You Need\" are eight scientists working at Google.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Who are the author of Attention is all you need ',\n",
       " 'output': 'The authors of the research paper \"Attention Is All You Need\" are eight scientists working at Google.'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executer.invoke({\"input\":\"Who are the author of Attention is all you need \"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `arxiv` with `{'query': '1605.08386'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mPublished: 2016-05-26\n",
      "Title: Heat-bath random walks with Markov bases\n",
      "Authors: Caprice Stanley, Tobias Windisch\n",
      "Summary: Graphs on lattice points are studied whose edges come from a finite set of\n",
      "allo\u001b[0m\u001b[32;1m\u001b[1;3mThe paper with the identifier 1605.08386 is titled \"Heat-bath random walks with Markov bases\" by Caprice Stanley and Tobias Windisch. The paper explores graphs on lattice points where the edges are derived from a finite set of allocations.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': \"what's the paper 1605.08386 about ?\",\n",
       " 'output': 'The paper with the identifier 1605.08386 is titled \"Heat-bath random walks with Markov bases\" by Caprice Stanley and Tobias Windisch. The paper explores graphs on lattice points where the edges are derived from a finite set of allocations.'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executer.invoke({\"input\":\"what's the paper 1605.08386 about ?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
